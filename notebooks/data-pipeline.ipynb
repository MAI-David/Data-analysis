{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Housekeeping",
   "id": "399d263540672cd0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Library imports",
   "id": "bb5060bfd5fba465"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from click.formatting import iter_rows\n",
    "\n",
    "if False:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install -r requirements.txt\n",
    "\n",
    "## This block did not effectively install modules from requirements file - done manually using \"pip install -r notebooks/requirements.txt\""
   ],
   "id": "2f99d3792cfe9adc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: remove unnecessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RepeatedStratifiedKFold,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from colorama import Fore, Back, Style\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from supertree import SuperTree"
   ],
   "id": "6ecf48aa267c9045",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Settings",
   "id": "8c656628fc03b647"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "sklearn.set_config(transform_output=\"pandas\")\n",
    "print(Style.RESET_ALL)"
   ],
   "id": "5f835385bd9767c5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Data imports\n",
    "Data was manually edited, to convert the mpa411.txt TSV format to a CSV format. Otherwise, Pandas was loading it as a single column, somehow. The first row, containing only \"#mpa_vJun23_CHOCOPhlAnSGB_202403\" was removed."
   ],
   "id": "5020c4643039c705"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data = pd.read_csv('../data/raw/MAI3004_lucki_mpa411.csv')\n",
    "metadata = pd.read_csv('../data/raw/MAI3004_lucki_metadata_safe.csv')\n",
    "print(\n",
    "    f\"Data successfully imported. \\n shape of data: {data.shape} \\n \"\n",
    "    f\"Shape of metadata: {metadata.shape}\"\n",
    ")\n",
    "\n",
    "assert data.shape == (6903, 932), \"Data has the wrong shape. Check the CSV formatting.\"\n",
    "assert metadata.shape == (930, 6), \"Metadata has the wrong shape. Check the CSV formatting.\"\n"
   ],
   "id": "c514adce339820c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Function definitions\n",
    "| Function Name | Description | Parameters |\n",
    "|---------------|-------------|------------|\n"
   ],
   "id": "de6543c4b1e67838"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Data preprocessing",
   "id": "8f3f8fffcdf7cd43"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Merge data and metadata",
   "id": "5576c8fffa6fa9a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_cols = [col for col in data.columns if col.startswith(\"mpa411_\")]\n",
    "\n",
    "sample_abundances = (\n",
    "    data[['clade_name'] + sample_cols]\n",
    "    .set_index('clade_name')\n",
    "    .transpose()\n",
    "    .rename_axis('original_sample_id')\n",
    "    .reset_index()\n",
    "    .rename(columns={'original_sample_id': 'sample_id'})\n",
    ")\n",
    "\n",
    "sample_abundances[\"sample_id\"] = (\n",
    "    sample_abundances[\"sample_id\"].str.removeprefix(\n",
    "        \"mpa411_\",\n",
    "    )\n",
    ")\n",
    "\n",
    "metadata_common = metadata[\n",
    "    metadata[\"sample_id\"].isin(sample_abundances[\"sample_id\"])\n",
    "].copy()\n",
    "merged_samples = metadata_common.merge(\n",
    "    sample_abundances,\n",
    "    on=\"sample_id\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "merged_samples.drop(columns=['year_of_birth', 'body_product'], inplace=True)\n",
    "# YOB and body_product are omitted without sample dates.\n",
    "# All samples are fecal.\n",
    "# TODO: should we be accounting for sex? Do statistical analysis\n",
    "\n",
    "print(f\"Metadata rows (original): {metadata.shape[0]}\")\n",
    "print(f\"Metadata rows with matching samples: {metadata_common.shape[0]}\")\n",
    "print(\n",
    "    f\"Metadata rows without matching samples: \"\n",
    "    f\"{metadata_common.shape[0]-metadata_common.shape[0]}\"\n",
    ")\n",
    "print(f\"Merged dataframe shape: {merged_samples.shape}\")"
   ],
   "id": "f2ba864552612c36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "merged_samples.head()",
   "id": "efaa682b453410d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Encoding",
   "id": "583e8784687d3504"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Sex and family_ID\n",
    "encoded_samples = merged_samples.copy().dropna(subset=\"age_group_at_sample\")\n",
    "\n",
    "encoded_samples[\"sex\"] = (\n",
    "    encoded_samples[\"sex\"]\n",
    "    .fillna(\"unknown\")\n",
    "    .replace({\"female\": 1, \"male\": 0, \"unknown\": 2})\n",
    ")\n",
    "encoded_samples[\"family_id\"] = LabelEncoder().fit_transform(\n",
    "    encoded_samples[\"family_id\"]\n",
    ")\n"
   ],
   "id": "113e90ec5ac431ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "encoding_guide = {\n",
    "    '1-2 weeks': 1,\n",
    "    '4 weeks': 2,\n",
    "    '8 weeks': 3,\n",
    "    '4 months': 4,\n",
    "    '5 months': 5,\n",
    "    '6 months': 6,\n",
    "    '9 months': 7,\n",
    "    '11 months': 8,\n",
    "    '14 months': 9,\n",
    "}\n",
    "encoded_samples[\"age_group_at_sample\"].replace(encoding_guide, inplace=True)\n",
    "\n",
    "# consider in interpretation that the distances between the real age bins are not the same as our age groups"
   ],
   "id": "4eb3daa5e3868cac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if False in pd.DataFrame(encoded_samples[\"age_group_at_sample\"]).applymap(np.isreal): #fallback encoder\n",
    "    age_encoder = LabelEncoder().fit(encoded_samples[\"age_group_at_sample\"])\n",
    "    encoded_samples[\"age_group_at_sample\"] = age_encoder.transform(\n",
    "        encoded_samples[\"age_group_at_sample\"]\n",
    "    )\n",
    "\n",
    "    age_groups = dict(\n",
    "        zip(age_encoder.classes_, age_encoder.transform(age_encoder.classes_))\n",
    "    )\n",
    "    print(\"Age group encoding:\", age_groups)\n",
    "\n",
    "else:\n",
    "    print(\"Fallback encoding not needed\")"
   ],
   "id": "e622535de0c6418c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Missing check",
   "id": "d8aac0ca570e48ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "missing_table = (\n",
    "    encoded_samples.isna()\n",
    "    .sum()\n",
    "    .to_frame(name=\"missing_count\")\n",
    "    .assign(\n",
    "        missing_percent=lambda df: (\n",
    "            (df[\"missing_count\"] / encoded_samples.shape[0] * 100).round(2)\n",
    "        ),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={\"index\": \"column\"})\n",
    "    .sort_values(\"missing_count\", ascending=False)\n",
    "    .query(\"missing_count != 0\")\n",
    ")\n",
    "\n",
    "if len(missing_table) > 0:\n",
    "    missing_table\n",
    "else:\n",
    "    print(\"No missing values detected.\")"
   ],
   "id": "2a3aeb028c98d944",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Outlier check",
   "id": "874dbd47290a3431"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "numeric_cols = encoded_samples.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "q1 = encoded_samples[numeric_cols].quantile(0.25)\n",
    "q3 = encoded_samples[numeric_cols].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "\n",
    "lower_bounds = q1 - 1.5 * iqr\n",
    "upper_bounds = q3 + 1.5 * iqr\n",
    "\n",
    "outlier_mask = (\n",
    "    (encoded_samples[numeric_cols] < lower_bounds)\n",
    "    | (encoded_samples[numeric_cols] > upper_bounds)\n",
    ")\n",
    "outlier_counts = outlier_mask.sum()\n",
    "outlier_percent = (outlier_counts / encoded_samples.shape[0] * 100).round(2)\n",
    "\n",
    "outlier_table = (\n",
    "    pd.DataFrame({\n",
    "        \"column\": numeric_cols,\n",
    "        \"lower_bound\": lower_bounds,\n",
    "        \"upper_bound\": upper_bounds,\n",
    "        \"outlier_count\": outlier_counts,\n",
    "        \"outlier_percent\": outlier_percent,\n",
    "    })\n",
    "    .query(\"outlier_count > 0\")\n",
    "    .sort_values(\"outlier_percent\", ascending=False)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "outlier_table"
   ],
   "id": "470119b1611b2728",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Normalisation check",
   "id": "7f341a63adf2391e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "normalized_samples = encoded_samples.copy()\n",
    "print(\"Shapiro-Wilk Normality Test\")\n",
    "\n",
    "for column in numeric_cols:\n",
    "    data_nona = normalized_samples[column].dropna()\n",
    "    stat, p_value = stats.shapiro(data_nona)\n",
    "\n",
    "    if p_value > 0.05:\n",
    "        print(Fore.GREEN + f\"{column}: Normally Distributed (p={p_value:.4f})\")\n",
    "\n",
    "    else:\n",
    "        print(\n",
    "            Fore.RED\n",
    "            + f\"{column}: Not Normally Distributed (p={p_value:.4f})\"\n",
    "        )\n",
    "\n",
    "print(Style.RESET_ALL)"
   ],
   "id": "a280b5e233c651b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Train-test split before pre-processing",
   "id": "abdad36c8dcb2dd2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "feature_cols = normalized_samples.columns.difference([\"sample_id\", \"age_group_at_sample\"]) # These variables will get removed from X\n",
    "\n",
    "X = normalized_samples[feature_cols]\n",
    "Y = normalized_samples[\"age_group_at_sample\"]\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=3004)\n",
    "train_indicies, test_indicies = next(gss.split(X, Y, groups=X['family_id']))\n",
    "X_train_raw = X.iloc[train_indicies]\n",
    "X_test_raw = X.iloc[test_indicies]\n",
    "Y_train = Y.iloc[train_indicies]\n",
    "Y_test = Y.iloc[test_indicies]\n",
    "\n",
    "assert X_train_raw.shape[1] == X_test_raw.shape[1], \"Feature columns do not match between train and test sets.\"\n",
    "assert X_train_raw.shape[0] == Y_train.shape[0] and X_test_raw.shape[0] == Y_test.shape[0], \"X and Y do not have the same length.\"\n",
    "\n",
    "print(\"Train shape:\", X_train_raw.shape, \"| Test shape:\", X_test_raw.shape)"
   ],
   "id": "58a6992773d5d07a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Normalising data using log",
   "id": "e3a1a725f7a3f655"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train = np.log1p(X_train_raw)\n",
    "X_test = np.log1p(X_test_raw)"
   ],
   "id": "12ac6f322cb3608b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#To activate back the raw data without normalisation\n",
    "\n",
    "#X_train = X_train_raw\n",
    "#X_test = X_test_raw"
   ],
   "id": "948579175e4f2108",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Shapiro-Wilk Normality Test (after Log Normlaisation)\\n\")\n",
    "\n",
    "for col in X_train.columns:\n",
    "\n",
    "    # 1. Get the data for this column from both sets\n",
    "    train_data = X_train[col].dropna()\n",
    "    test_data = X_test[col].dropna()\n",
    "\n",
    "    # 2. Run Shapiro test on both\n",
    "    stat_train, p_train = stats.shapiro(train_data)\n",
    "    stat_test, p_test = stats.shapiro(test_data)\n",
    "\n",
    "    # 3. Determine status (Both must be > 0.05 to be truly \"Normal\")\n",
    "    is_train_normal = p_train > 0.05\n",
    "    is_test_normal = p_test > 0.05\n",
    "\n",
    "    # 4. Print Logic\n",
    "    # If both are Green\n",
    "    if is_train_normal and is_test_normal:\n",
    "        print(Fore.GREEN + f\"✔ {col}: Normal (Train p={p_train:.3f}, Test p={p_test:.3f})\")\n",
    "\n",
    "    # If one is Red (Mixed results)\n",
    "    elif is_train_normal or is_test_normal:\n",
    "        print(Fore.YELLOW + f\"⚠ {col}: Inconsistent (Train p={p_train:.3f}, Test p={p_test:.3f})\")\n",
    "\n",
    "    # If both are Red\n",
    "    else:\n",
    "        print(Fore.RED + f\"✘ {col}: Not Normal (Train p={p_train:.3f}, Test p={p_test:.3f})\")\n",
    "\n",
    "print(Style.RESET_ALL)"
   ],
   "id": "4ae08ebb1d3eedcb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Exploratory data analysis",
   "id": "dabfc513025c4f1f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(merged_samples.shape)\n",
    "merged_samples.head()"
   ],
   "id": "ba68ecc25cac09ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dataset overview\n",
    "print(\"Number of samples:\", len(merged_samples))\n",
    "print(\n",
    "    \"Number of unique families (family_id):\",\n",
    "    merged_samples[\"family_id\"].nunique(),\n",
    ")\n",
    "print(\"Number of columns (metadata + features):\", merged_samples.shape[1])"
   ],
   "id": "606176607d03a4b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Samples per family\n",
    "samples_per_family = merged_samples[\"family_id\"].value_counts()\n",
    "samples_per_family.describe()"
   ],
   "id": "6b9e9f9970b7d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "samples_per_family.hist(bins=20)\n",
    "plt.xlabel(\"Number of samples per family\")\n",
    "plt.ylabel(\"Number of families\")\n",
    "plt.title(\"Distribution of samples per family\")\n",
    "plt.show()"
   ],
   "id": "7e7c3be1a10fc5fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#distribution of age groups\n",
    "merged_samples[\"age_group_at_sample\"].value_counts(dropna=False)"
   ],
   "id": "d9329aff78d5b3c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "merged_samples[\"age_group_at_sample\"].value_counts().plot(kind=\"bar\")\n",
    "plt.title(\"Distribution of age groups\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "id": "51d9ecaf892191e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#dimensionality and sparsity of the microbiome feature matrix\n",
    "metadata_cols = [\n",
    "    \"sample_id\",\n",
    "    \"family_id\",\n",
    "    \"sex\",\n",
    "    \"body_product\",\n",
    "    \"age_group_at_sample\",\n",
    "    \"year_of_birth\",\n",
    "]\n",
    "feature_cols = [c for c in merged_samples.columns if c not in metadata_cols]\n",
    "\n",
    "X = merged_samples[feature_cols]\n",
    "\n",
    "print(\"Feature matrix shape:\", X.shape)\n",
    "print(\"Overall fraction of zeros:\", (X == 0).mean().mean())"
   ],
   "id": "e06174f65cc8e34b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#number of observed taxa per sample\n",
    "nonzero_per_sample = (X > 0).sum(axis=1)\n",
    "nonzero_per_sample.describe()"
   ],
   "id": "1b3acaedd5cc3fa5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nonzero_per_sample.hist(bins=50)\n",
    "plt.xlabel(\"Number of non-zero taxa per sample\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.title(\"Non-zero taxa per sample\")\n",
    "plt.show()"
   ],
   "id": "65dad520b0d77b51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Total abundance per sample (sanity check)\n",
    "total_abundance = X.sum(axis=1)\n",
    "total_abundance.describe()"
   ],
   "id": "2b343c13e583d33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "total_abundance.hist(bins=50)\n",
    "plt.xlabel(\"Total abundance per sample\")\n",
    "plt.ylabel(\"Number of samples\")\n",
    "plt.title(\"Total microbial abundance per sample\")\n",
    "plt.show()"
   ],
   "id": "6907499a19a59aff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Distribution of feature prevalence\n",
    "feature_prevalence = (X > 0).sum(axis=0)\n",
    "feature_prevalence.describe()"
   ],
   "id": "4d3aaf79594e046",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "feature_prevalence.hist(bins=50)\n",
    "plt.xlabel(\"Number of samples in which taxon is present\")\n",
    "plt.ylabel(\"Number of taxa\")\n",
    "plt.title(\"Feature prevalence distribution\")\n",
    "plt.show()"
   ],
   "id": "a0ffef511eb970f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Distribution of non-zero abundances (log scale)\n",
    "\n",
    "nonzero_values = X.values[X.values > 0]\n",
    "plt.hist(np.log10(nonzero_values), bins=50)\n",
    "plt.xlabel(\"log10(abundance)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of non-zero abundances (log10 scale)\")\n",
    "plt.show()"
   ],
   "id": "1b7296031bd55a89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# PCA visualization\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use a subset of features for speed\n",
    "prevalence = (X > 0).sum(axis=0)\n",
    "top_features = prevalence.sort_values(ascending=False).head(500).index\n",
    "\n",
    "X_sub = X[top_features]\n",
    "\n",
    "# Scale features\n",
    "X_scaled = StandardScaler().fit_transform(X_sub)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Plot\n",
    "age = merged_samples[\"age_group_at_sample\"]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca.iloc[:, 0], X_pca.iloc[:, 1],\n",
    "            c=pd.factorize(age)[0], cmap=\"viridis\", alpha=0.6)\n",
    "plt.colorbar(label=\"Age group (encoded)\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA of samples colored by age group\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)"
   ],
   "id": "6576dbac7f5226c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary of EDA\n",
    "The dataset consists of 930 stool samples derived from multiple individuals across different families and contains approximately 6,900 microbiome features, making it a high-dimensional and highly sparse dataset. Each sample contains on average around 300 detected taxa, while the total microbial abundance per sample is relatively stable, indicating that sequencing depth is consistent across samples.\n",
    "Most taxa are rare and occur in only a small fraction of samples, whereas a small subset of taxa is highly prevalent across the cohort. The distribution of non-zero abundances follows an approximately log-normal shape, which is typical for microbiome sequencing data (e.g., Lutz et al., 2022).\n",
    "A PCA projection based on the most prevalent taxa does not reveal sharply separated clusters but shows a gradual age-related gradient, suggesting that age-related variation in microbiome composition is present but represents only a limited fraction of the total variance in the data."
   ],
   "id": "b2701f775de15a2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Training",
   "id": "ed7a7dce85b7c13c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Features used by model:\")\n",
    "print(X_train_genus.columns.tolist())"
   ],
   "id": "4d9c47380284a8f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "\"target_names\" in X_train_genus.columns",
   "id": "86f7c04bf0752651",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filtering for features at the genus level",
   "id": "6cf7a10ce0a34b45"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def filter_genus(df_uf): #Defining a function that filters a dataframe to only include columns with features at genus level\n",
    "    df_uf = df_uf.drop(list(df_uf.filter(regex=\"s__\")),axis=1,inplace=False) #Drops columns that include features at species level\n",
    "    df_uf = df_uf.filter(regex=\"g__\") #Drops columns that include features broader than genus level\n",
    "    return df_uf"
   ],
   "id": "4ca632e5f2be36c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_train_genus = filter_genus(X_train)\n",
    "X_test_genus = filter_genus(X_test)"
   ],
   "id": "5f9a719e54f0d525",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Random Forest Regressor with Train/Test split (Genus)",
   "id": "75b6043b1a0137aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Base model",
   "id": "54395779c8be692e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create model\n",
    "# Base model\n",
    "rf = RandomForestRegressor(\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    oob_score=True\n",
    ")"
   ],
   "id": "b5fe872fbe03cbb4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Search for the best model",
   "id": "5c770d9cdf1a6bee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameter space\n",
    "param_dist = {\n",
    "    \"n_estimators\": [300, 500, 800, 1000],\n",
    "    \"max_depth\": [None, 10, 20, 40],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [1, 2, 5, 10],\n",
    "    \"max_features\": [\"sqrt\", \"log2\"]\n",
    "}\n",
    "\n",
    "# Randomized search with 5-fold CV on training set\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=25,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Fit search\n",
    "search.fit(X_train_genus, Y_train)\n",
    "\n",
    "# Best model\n",
    "best_model = search.best_estimator_\n",
    "print(\"\\nBest hyperparameters:\", search.best_params_)"
   ],
   "id": "1f6fd166588f5dfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# evaluate model\n",
    "yhat = best_model.predict(X_test_genus)\n",
    "mse = mean_squared_error(Y_test, yhat)\n",
    "print('Mean Squared Error: %.3f' % mse)\n",
    "\n",
    "# CV RMSE of best model\n",
    "best_cv_rmse = (-search.best_score_) ** 0.5\n",
    "print(\"Best CV RMSE:\", best_cv_rmse)\n",
    "\n",
    "#R-squared\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(Y_test, yhat)\n",
    "print('R2 Score: %.3f' % r2)"
   ],
   "id": "6f808e321be8615a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "sns.scatterplot(x=Y_test, y=yhat, alpha=0.6)\n",
    "plt.plot([Y_test.min(), Y_test.max()],\n",
    "         [Y_test.min(), Y_test.max()],\n",
    "         color=\"red\", linestyle=\"--\")\n",
    "\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Predicted vs Actual (Genus-level RF)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "2768e690a0cc9ad4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "residuals = Y_test - yhat\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x=yhat, y=residuals, alpha=0.6)\n",
    "plt.axhline(0, linestyle=\"--\", color=\"red\")\n",
    "\n",
    "plt.xlabel(\"Predicted values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals vs Predictions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "863bde6c829f4700",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "importances = pd.Series(\n",
    "    best_model.feature_importances_,\n",
    "    index=X_train_genus.columns\n",
    ").sort_values(ascending=False)\n",
    "\n",
    "top_n = 20\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.barplot(\n",
    "    x=importances.head(top_n),\n",
    "    y=importances.head(top_n).index\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Feature importance\")\n",
    "plt.ylabel(\"Genus\")\n",
    "plt.title(f\"Top {top_n} most important genera\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "a47f420e5014a572",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "st = SuperTree(\n",
    "    best_model,\n",
    "    X_train_genus,\n",
    "    Y_train\n",
    ")\n",
    "\n",
    "st.show_tree(which_tree=0)"
   ],
   "id": "9a63dfab792725f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get predictions from each tree on the test set\n",
    "all_tree_preds = np.array([tree.predict(X_test_genus) for tree in best_model.estimators_])\n",
    "\n",
    "# compute the mean prediction (Random Forest final prediction)\n",
    "rf_pred = all_tree_preds.mean(axis=0)\n",
    "\n",
    "# compute standard deviation per sample (uncertainty)\n",
    "rf_std = all_tree_preds.std(axis=0)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "\n",
    "# plot all tree predictions (semi-transparent lines)\n",
    "for i in range(all_tree_preds.shape[0]):\n",
    "    plt.plot(Y_test.values, all_tree_preds[i], 'o', color='lightgray', alpha=0.3)\n",
    "\n",
    "# plot Random Forest mean prediction\n",
    "plt.scatter(Y_test, rf_pred, color='blue', label='RF mean prediction', s=40)\n",
    "\n",
    "plt.errorbar(Y_test, rf_pred, yerr=rf_std, fmt='o', color='red', alpha=0.5, label='±1 std across trees')\n",
    "\n",
    "plt.plot([Y_test.min(), Y_test.max()],\n",
    "         [Y_test.min(), Y_test.max()],\n",
    "         color='black', linestyle='--', label='Perfect prediction')\n",
    "\n",
    "plt.xlabel(\"Actual Age Group\")\n",
    "plt.ylabel(\"Predicted Age Group\")\n",
    "plt.title(\"Random Forest – Forest plot of tree predictions\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "5cea6fd6c8f853d0",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
